{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JfLUlawto_D"
   },
   "source": [
    "# Classification on Energy data\n",
    "## This code was programmed on Google Colab.\n",
    "# ** READ.ME **\n",
    "## ***Please run this file on Google Colab. And run step by step.*** If you open this file through Jupyter Notebook, it might not run through, since the indentation is slightly different. \n",
    "## ***You have to choose file on the 5th code cell.*** I attached the file in zip file.\n",
    "## I used Google colab because I had a problem in updating Tensorflow to the latest version. Thank you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mthoSGBAOoX-"
   },
   "source": [
    "\n",
    "\n",
    "This file contains code to:\n",
    "\n",
    "* Load a CSV file using Pandas.\n",
    "* Create train, validation, and test sets.\n",
    "* Define and train a model using Keras (including setting class weights).\n",
    "* Evaluate the model using various metrics (including precision and recall).\n",
    "* Try common tecniques for dealing with imbalanced data like:\n",
    "    * Class weighting \n",
    "    * Oversampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRHmSyHxEIhN"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJHVo_K_v20i"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fYBlUQ5FvzxP"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JM7hDSNClfoK"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c8o1FHzD-_y_"
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3iZVjziKHmX"
   },
   "source": [
    "## Data processing and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eJd6ArRR_MV-"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fw6HB_92_Mhm"
   },
   "outputs": [],
   "source": [
    "#--- Store Data set in Pandas Data Frame\n",
    "import io\n",
    "raw_df = pd.read_csv(io.BytesIO(uploaded['energydata_complete.csv']))\n",
    "# Dataset is now stored in a Pandas Dataframe\n",
    "raw_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5l4swJEg_eWU"
   },
   "outputs": [],
   "source": [
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "inO-FyAwAlIE"
   },
   "outputs": [],
   "source": [
    "#--- According to our Problem setting, Make y as Binary label (0,1)\n",
    "\n",
    "class_div = raw_df['Appliances'].median()\n",
    "\n",
    "App = []\n",
    "for yi in raw_df['Appliances']:\n",
    "    if yi >= class_div :\n",
    "        App.append(1)\n",
    "    else :\n",
    "        App.append(0)\n",
    "\n",
    "raw_df['Class'] = App\n",
    "raw_df = raw_df.drop(['Appliances'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWKB_CVZFLpB"
   },
   "source": [
    "### Examine the class label imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCJFrtuY2iLF"
   },
   "outputs": [],
   "source": [
    "neg, pos = np.bincount(raw_df['Class'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KnLKFQDsCBUg"
   },
   "source": [
    "This shows the smaller fraction of negative samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6qox6ryyzwdr"
   },
   "source": [
    "### Clean, split and normalize the data\n",
    "\n",
    "The raw data has a few issues. First the `Time` and `Amount` columns are too variable to use directly. Drop the `Time` column (since it's not clear what it means) and take the log of the `Ammount` column to reduce its range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-K6T457FBxhJ"
   },
   "outputs": [],
   "source": [
    "cleaned_df = raw_df.copy()\n",
    "\n",
    "#--- Delete columns we'll not use\n",
    "labels_to_drop = ['date','lights','rv1', 'rv2']\n",
    "cleaned_df = cleaned_df.drop(labels=labels_to_drop, axis=1) # df.drop(): non-fruitful function\n",
    "cleaned_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uSNgdQFFFQ6u"
   },
   "source": [
    "Split the dataset into train, validation, and test sets. The validation set is used during the model fitting to evaluate the loss and any metrics, however the model is not fit with this data. The test set is completely unused during the training phase and is only used at the end to evaluate how well the model generalizes to new data. This is especially important with imbalanced datasets where [overfitting](https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting) is a significant concern from the lack of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xfxhKg7Yr1-b"
   },
   "outputs": [],
   "source": [
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, test_df = train_test_split(cleaned_df, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('Class'))\n",
    "bool_train_labels = train_labels != 0\n",
    "val_labels = np.array(val_df.pop('Class'))\n",
    "test_labels = np.array(test_df.pop('Class'))\n",
    "\n",
    "train_features = np.array(train_df)\n",
    "val_features = np.array(val_df)\n",
    "test_features = np.array(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T9s94bbQDz4C"
   },
   "outputs": [],
   "source": [
    "val_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8a_Z_kBmr7Oh"
   },
   "source": [
    "Normalize the input features using the sklearn StandardScaler.\n",
    "This will set the mean to 0 and standard deviation to 1.\n",
    "\n",
    "Note: The `StandardScalar` is only fit using the `train_features` to be sure the model is not peeking at the validation or test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IO-qEUmJ5JQg"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "train_features = np.clip(train_features, -5, 5)\n",
    "val_features = np.clip(val_features, -5, 5) \n",
    "test_features = np.clip(test_features, -5, 5)\n",
    "\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Test features shape:', test_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XF2nNfWKJ33w"
   },
   "source": [
    "Caution: If you want to deploy a model, it's critical that you preserve the preprocessing calculations. The easiest way to implement them as layers, and attach them to your model before export.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQ9teG5xvnkK"
   },
   "source": [
    "### Look at the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "raK7hyjd_vf6"
   },
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame(train_features[ bool_train_labels], columns = train_df.columns)\n",
    "neg_df = pd.DataFrame(train_features[~bool_train_labels], columns = train_df.columns)\n",
    "\n",
    "sns.jointplot(pos_df['T6'], pos_df['T9'], #T9 can be deleted\n",
    "              kind='hex', xlim = (-5,5), ylim = (-5,5))\n",
    "#plt.suptitle(\"Positive distribution\")\n",
    "\n",
    "sns.jointplot(neg_df['T6'], neg_df['T9'],\n",
    "              kind='hex', xlim = (-5,5), ylim = (-5,5))\n",
    "#_ = plt.suptitle(\"Negative distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFK1u4JX16D8"
   },
   "source": [
    "## Define the model and metrics\n",
    "\n",
    "Define a function that creates a simple neural network with a densly connected hidden layer, a [dropout](https://developers.google.com/machine-learning/glossary/#dropout_regularization) layer to reduce overfitting, and an output sigmoid layer that returns the probability of a transaction being fraudulent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3JQDzUqT3UYG"
   },
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "Activation_f = ['relu']\n",
    "\n",
    "def make_model(metrics = METRICS, output_bias=None, activation_f=Activation_f):\n",
    "  if output_bias is not None:\n",
    "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "  for func in Activation_f:\n",
    "    model = keras.Sequential([\n",
    "      keras.layers.Dense(\n",
    "          24, activation='relu',\n",
    "          input_shape=(train_features.shape[-1],)),\n",
    "      keras.layers.Dense(12, activation=func),\n",
    "      keras.layers.Dropout(0.5),\n",
    "      keras.layers.Dense(1, activation='sigmoid',\n",
    "                         bias_initializer=output_bias),\n",
    "  ])\n",
    "\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=metrics)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ohSP-69dU4wE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SU0GX6E6mieP"
   },
   "source": [
    "### Understanding useful metrics\n",
    "\n",
    "Notice that there are a few metrics defined above that can be computed by the model that will be helpful when evaluating the performance.\n",
    "\n",
    "\n",
    "\n",
    "*   **False** negatives and **false** positives are samples that were **incorrectly** classified\n",
    "*   **True** negatives and **true** positives are samples that were **correctly** classified\n",
    "*   **Accuracy** is the percentage of examples correctly classified\n",
    ">   $\\frac{\\text{true samples}}{\\text{total samples}}$\n",
    "*   **Precision** is the percentage of **predicted** positives that were correctly classified\n",
    ">   $\\frac{\\text{true positives}}{\\text{true positives + false positives}}$\n",
    "*   **Recall** is the percentage of **actual** positives that were correctly classified\n",
    ">   $\\frac{\\text{true positives}}{\\text{true positives + false negatives}}$\n",
    "*   **AUC** refers to the Area Under the Curve of a Receiver Operating Characteristic curve (ROC-AUC). This metric is equal to the probability that a classifier will rank a random positive sample higher than than a random negative sample.\n",
    "\n",
    "Note: Accuracy is not a helpful metric for this task. You can 99.8%+ accuracy on this task by predicting False all the time.  \n",
    "\n",
    "Read more:\n",
    "*  [True vs. False and Positive vs. Negative](https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative)\n",
    "*  [Accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy)\n",
    "*   [Precision and Recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)\n",
    "*   [ROC-AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FYdhSAoaF_TK"
   },
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IDbltVPg2m2q"
   },
   "source": [
    "### Build the model\n",
    "\n",
    "Now create and train your model using the function that was defined earlier. Notice that the model is fit using a larger than default batch size of 2048, this is important to ensure that each batch has a decent chance of containing a few positive samples. If the batch size was too small, they would likely have no fraudelent transactions to learn from.\n",
    "\n",
    "\n",
    "Note: this model will not handle the class imbalance well. You will improve it later in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ouUkwPcGQsy3"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 200 \n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xlR_dekzw7C"
   },
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6NU3XGNQVEXa"
   },
   "outputs": [],
   "source": [
    "Activation_f = ['tanh']\n",
    "\n",
    "model2 = make_model()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TrF08ZRnVOeI"
   },
   "outputs": [],
   "source": [
    "Activation_f = ['softmax']\n",
    "\n",
    "model3 = make_model()\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wx7ND3_SqckO"
   },
   "source": [
    "Test run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LopSd-yQqO3a"
   },
   "outputs": [],
   "source": [
    "model.predict(train_features[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKIgWqHms_03"
   },
   "source": [
    "### Optional: Set the correct initial bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdbfWDuVpo6k"
   },
   "source": [
    "With the default bias initialization the loss should be about `math.log(2) = 0.69314` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H-oPqh3SoGXk"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "print(\"Loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2xuhnTx1wDUP"
   },
   "source": [
    "It is not very different from ln(2) value.\n",
    "But to experiment, we will try to find the correct bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hE-JRzfKqfhB"
   },
   "source": [
    "\n",
    "\n",
    "The correct bias to set can be derived from:\n",
    "\n",
    "$$ p_0 = pos/(pos + neg) = 1/(1+e^{-b_0}) $$\n",
    "$$ b_0 = -log_e(1/p_0 - 1) $$\n",
    "$$ b_0 = log_e(pos/neg)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5KWPSjjstUS"
   },
   "outputs": [],
   "source": [
    "initial_bias = np.log([pos/neg])\n",
    "initial_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d1juXI9yY1KD"
   },
   "source": [
    "Set that as the initial bias, and the model will give much more reasonable initial guesses. \n",
    "\n",
    "It should be near: `pos/total = 0.0018`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50oyu1uss0i-"
   },
   "outputs": [],
   "source": [
    "model = make_model(output_bias = initial_bias)\n",
    "model.predict(train_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVDqCWXDqHSc"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "print(\"Loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FrDC8hvNr9yw"
   },
   "source": [
    "This initial loss is less than if would have been with naive initilization.\n",
    "\n",
    "This way the model doesn't need to spend the first few epochs just learning that positive examples are unlikely. This also makes it easier to read plots of the loss during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0EJj9ixKVBMT"
   },
   "source": [
    "### Checkpoint the initial weights\n",
    "\n",
    "To make the various training runs more comparable, keep this initial model's weights in a checkpoint file, and load them into each model before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tSUm4yAVIif"
   },
   "outputs": [],
   "source": [
    "initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\n",
    "model.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EVXiLyqyZ8AX"
   },
   "source": [
    "### Confirm that the bias fix helps\n",
    "\n",
    "Before moving on, confirm quick that the careful bias initialization actually helped.\n",
    "\n",
    "Train the model for 20 epochs, with and without this careful initialization, and compare the losses: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dm4-4K5RZ63Q"
   },
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "model.load_weights(initial_weights)\n",
    "model.layers[-1].bias.assign([0.0])\n",
    "zero_bias_history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    validation_data=(val_features, val_labels), \n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j8DsLXHQaSql"
   },
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "model.load_weights(initial_weights)\n",
    "careful_bias_history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    validation_data=(val_features, val_labels), \n",
    "    verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E3XsMBjhauFV"
   },
   "outputs": [],
   "source": [
    "def plot_loss(history, label, n):\n",
    "  # Use a log scale to show the wide range of values.\n",
    "  plt.semilogy(history.epoch,  history.history['loss'],\n",
    "               color=colors[n], label='Train '+label)\n",
    "  plt.semilogy(history.epoch,  history.history['val_loss'],\n",
    "          color=colors[n], label='Val '+label,\n",
    "          linestyle=\"--\")\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  \n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "twzEIMhtWpwi"
   },
   "outputs": [],
   "source": [
    "plot_loss(zero_bias_history, \"Zero Bias\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dxFaskm7beC7"
   },
   "outputs": [],
   "source": [
    "plot_loss(zero_bias_history, \"Zero Bias\", 0)\n",
    "plot_loss(careful_bias_history, \"Careful Bias\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fKMioV0ddG3R"
   },
   "source": [
    "The above figure makes it clear: In terms of validation loss, on this problem, this careful initialization gives an advantage, but not a lot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkUON-a-Ve9j"
   },
   "outputs": [],
   "source": [
    "\n",
    "model2.load_weights(initial_weights)\n",
    "model2.layers[-1].bias.assign([0.0])\n",
    "zero_bias_history = model2.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    validation_data=(val_features, val_labels), \n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "we4Kd-uPVfBr"
   },
   "outputs": [],
   "source": [
    "def plot_loss(history, label, n):\n",
    "  # Use a log scale to show the wide range of values.\n",
    "  plt.semilogy(history.epoch,  history.history['loss'],\n",
    "               color=colors[n], label='Train '+label)\n",
    "  plt.semilogy(history.epoch,  history.history['val_loss'],\n",
    "          color=colors[n], label='Val '+label,\n",
    "          linestyle=\"--\")\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  \n",
    "  plt.legend()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ak7AIjARVq80"
   },
   "outputs": [],
   "source": [
    "plot_loss(zero_bias_history, \"Zero Bias\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MiOZPCqHV-GR"
   },
   "outputs": [],
   "source": [
    "\n",
    "model3.load_weights(initial_weights)\n",
    "model3.layers[-1].bias.assign([0.0])\n",
    "zero_bias_history = model3.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    validation_data=(val_features, val_labels), \n",
    "    verbose=0)\n",
    "\n",
    "plot_loss(zero_bias_history, \"Zero Bias\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ApKrKnHV-CT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L7rbk2hlV9_W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RsA_7SEntRaV"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZKAc8NCDnoR"
   },
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "model.load_weights(initial_weights)\n",
    "baseline_history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iSaDBYU9xtP6"
   },
   "source": [
    "### Check training history\n",
    "In this section, you will produce plots of your model's accuracy and loss on the training and validation set. These are useful to check for overfitting.\n",
    "Additionally, you can produce these plots for any of the metrics you created above. False negatives are included as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WTSkhT1jyGu6"
   },
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "  metrics =  ['loss', 'auc', 'precision', 'recall']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch,  history.history[metric], color=colors[n], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[n], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0.4,0.7]) #plt.ylim()[2]\n",
    "    elif metric == 'auc':\n",
    "      plt.ylim([0.5,1])\n",
    "    else:\n",
    "      plt.ylim([0.4,1.01])\n",
    "\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6LReDsqlZlk"
   },
   "outputs": [],
   "source": [
    "plot_metrics(baseline_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UCa4iWo6WDKR"
   },
   "source": [
    "Note: That the validation curve sometimes performs better than the training curve. This is mainly caused by the fact that the dropout layer is not active when evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mz3q9wzmJ8y1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJC1booryouo"
   },
   "source": [
    "### Evaluate metrics\n",
    "\n",
    "You can use a [confusion matrix](https://developers.google.com/machine-learning/glossary/#confusion_matrix) to summarize the actual vs. predicted labels where the X axis is the predicted label and the Y axis is the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aNS796IJKrev"
   },
   "outputs": [],
   "source": [
    "train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVWBGfADwbWI"
   },
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, p=0.5):\n",
    "  cm = confusion_matrix(labels, predictions > p)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('Low Energy Usages Detected (True Negatives): ', cm[0][0])\n",
    "  print('Low Energy Usages Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "  print('High Energy Usages Missed (False Negatives): ', cm[1][0])\n",
    "  print('High Energy Usages Detected (True Positives): ', cm[1][1])\n",
    "  print('Total High Energy Usages Households: ', np.sum(cm[1]))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nOTjD5Z5Wp1U"
   },
   "source": [
    "Evaluate your model on the test dataset and display the results for the metrics you created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "poh_hZngt2_9"
   },
   "outputs": [],
   "source": [
    "baseline_results = model.evaluate(test_features, test_labels,\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(test_labels, test_predictions_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-QpQsip_F2Q"
   },
   "source": [
    "### Plot the ROC\n",
    "\n",
    "Now plot the [ROC](https://developers.google.com/machine-learning/glossary#ROC). This plot is useful because it shows, at a glance, the range of performance the model can reach just by tuning the output threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lhaxsLSvANF9"
   },
   "outputs": [],
   "source": [
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "  plt.xlabel('False positives [%]')\n",
    "  plt.ylabel('True positives [%]')\n",
    "  plt.xlim([-0.5,100])\n",
    "  plt.ylim([0,100.5])\n",
    "  plt.grid(True)\n",
    "  ax = plt.gca()\n",
    "  ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DfHHspttKJE0"
   },
   "outputs": [],
   "source": [
    "plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color=colors[0])\n",
    "plot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\n",
    "plt.plot(np.arange(100),np.arange(100),ls=':',alpha=0.5) # x=y\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cveQoiMyGQCo"
   },
   "source": [
    "## Class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePGp6GUE1WfH"
   },
   "source": [
    "### Calculate class weights\n",
    "\n",
    "The goal is to identify Low Energy Usage, but you don't have very many of those positive samples to work with, so you would want to have the classifier heavily weight the few examples that are available. You can do this by passing Keras weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjGWErngGny7"
   },
   "outputs": [],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mk1OOE2ZSHzy"
   },
   "source": [
    "### Train a model with class weights\n",
    "\n",
    "Now try re-training and evaluating the model with class weights to see how that affects the predictions.\n",
    "\n",
    "Note: Using `class_weights` changes the range of the loss. This may affect the stability of the training depending on the optimizer. Optimizers who's step size is dependent on the magnitude of the gradient, like `optimizers.SGD`, may fail. The optimizer used here, `optimizers.Adam`, is unaffected by the scaling change. Also note that because of the weighting, the total losses are not comparable between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJ589fn8ST3x"
   },
   "outputs": [],
   "source": [
    "weighted_model = make_model()\n",
    "weighted_model.load_weights(initial_weights)\n",
    "\n",
    "weighted_history = weighted_model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(val_features, val_labels),\n",
    "    # The class weights go here\n",
    "    class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0ynYRO0G3Lx"
   },
   "source": [
    "### Check training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BBe9FMO5ucTC"
   },
   "outputs": [],
   "source": [
    "plot_metrics(weighted_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REy6WClTZIwQ"
   },
   "source": [
    "### Evaluate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nifqscPGw-5w"
   },
   "outputs": [],
   "source": [
    "train_predictions_weighted = weighted_model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_weighted = weighted_model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owKL2vdMBJr6"
   },
   "outputs": [],
   "source": [
    "weighted_results = weighted_model.evaluate(test_features, test_labels,\n",
    "                                           batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(weighted_model.metrics_names, weighted_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(test_labels, test_predictions_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PTh1rtDn8r4-"
   },
   "source": [
    "Here you can see that with class weights the accuracy and recall are lower because there are more false negatives, but conversely the precision and AUC are higher because the model also found more true positives. Despite having lower accuracy, this model has higher precision (and identifies more lower energy usage). Of course, there is a cost to both types of error (you wouldn't want to bug users by flagging too many low energy usages as high energy usages, either). We need to carefully consider the trade offs between these different types of errors for the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXDAwyr0HYdX"
   },
   "source": [
    "### Plot the ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3hzScIVZS1Xm"
   },
   "outputs": [],
   "source": [
    "plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color=colors[0])\n",
    "plot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\n",
    "\n",
    "plot_roc(\"Train Weighted\", train_labels, train_predictions_weighted, color=colors[1])\n",
    "plot_roc(\"Test Weighted\", test_labels, test_predictions_weighted, color=colors[1], linestyle='--')\n",
    "\n",
    "plt.plot(np.arange(100),np.arange(100),ls=':',alpha=0.5) # x=y\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ysRtr6xHnXP"
   },
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "18VUHNc-UF5w"
   },
   "source": [
    "### Oversample the minority class\n",
    "\n",
    "A related approach would be to resample the dataset by oversampling the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHirNp6u7OWp"
   },
   "outputs": [],
   "source": [
    "pos_features = train_features[bool_train_labels]\n",
    "neg_features = train_features[~bool_train_labels]\n",
    "\n",
    "pos_labels = train_labels[bool_train_labels]\n",
    "neg_labels = train_labels[~bool_train_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WgBVbX7P7QrL"
   },
   "source": [
    "#### Using NumPy\n",
    "\n",
    "You can balance the dataset manually by choosing the right number randon \n",
    "indices from the positive examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUzGjSkwqT88"
   },
   "outputs": [],
   "source": [
    "ids = np.arange(len(pos_features))\n",
    "choices = np.random.choice(ids, len(neg_features))\n",
    "\n",
    "res_pos_features = pos_features[choices]\n",
    "res_pos_labels = pos_labels[choices]\n",
    "\n",
    "res_pos_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ie_FFet6cep"
   },
   "outputs": [],
   "source": [
    "resampled_features = np.concatenate([res_pos_features, neg_features], axis=0)\n",
    "resampled_labels = np.concatenate([res_pos_labels, neg_labels], axis=0)\n",
    "\n",
    "order = np.arange(len(resampled_labels))\n",
    "np.random.shuffle(order)\n",
    "resampled_features = resampled_features[order]\n",
    "resampled_labels = resampled_labels[order]\n",
    "\n",
    "resampled_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IYfJe2Kc-FAz"
   },
   "source": [
    "#### Using `tf.data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "usyixaST8v5P"
   },
   "source": [
    "If you're using `tf.data` the easiest way to produce balanced examples is to start with a `positive` and a `negative` dataset, and merge them. See [the tf.data guide](../../guide/data.ipynb) for more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yF4OZ-rI6xb6"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000\n",
    "\n",
    "def make_ds(features, labels):\n",
    "  ds = tf.data.Dataset.from_tensor_slices((features, labels))#.cache()\n",
    "  ds = ds.shuffle(BUFFER_SIZE).repeat()\n",
    "  return ds\n",
    "\n",
    "pos_ds = make_ds(pos_features, pos_labels)\n",
    "neg_ds = make_ds(neg_features, neg_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNQUx-OA-oJc"
   },
   "source": [
    "Each dataset provides `(feature, label)` pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "llXc9rNH7Fbz"
   },
   "outputs": [],
   "source": [
    "for features, label in pos_ds.take(1):\n",
    "  print(\"Features:\\n\", features.numpy())\n",
    "  print()\n",
    "  print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLEfjZO0-vbN"
   },
   "source": [
    "Merge the two together using `experimental.sample_from_datasets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e7w9UQPT9wzE"
   },
   "outputs": [],
   "source": [
    "resampled_ds = tf.data.experimental.sample_from_datasets([pos_ds, neg_ds], weights=[0.5, 0.5])\n",
    "resampled_ds = resampled_ds.batch(BATCH_SIZE).prefetch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWXARdTdAuQK"
   },
   "outputs": [],
   "source": [
    "for features, label in resampled_ds.take(1):\n",
    "  print(label.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irgqf3YxAyN0"
   },
   "source": [
    "To use this dataset, you'll need the number of steps per epoch.\n",
    "\n",
    "The definition of \"epoch\" in this case is less clear. Say it's the number of batches required to see each negative example once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xH-7K46AAxpq"
   },
   "outputs": [],
   "source": [
    "resampled_steps_per_epoch = np.ceil(2.0*neg/BATCH_SIZE)\n",
    "resampled_steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XZ1BvEpcBVHP"
   },
   "source": [
    "### Train on the oversampled data\n",
    "\n",
    "Now try training the model with the resampled data set instead of using class weights to see how these methods compare.\n",
    "\n",
    "Note: Because the data was balanced by replicating the positiver examples, the total dataset size is larger, and each epoch runs for more training steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "soRQ89JYqd6b"
   },
   "outputs": [],
   "source": [
    "resampled_model = make_model()\n",
    "resampled_model.load_weights(initial_weights)\n",
    "\n",
    "# Reset the bias to zero, since this dataset is balanced.\n",
    "output_layer = resampled_model.layers[-1] \n",
    "output_layer.bias.assign([0])\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_features, val_labels)).cache()\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(2) \n",
    "\n",
    "resampled_history = resampled_model.fit(\n",
    "    resampled_ds,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=resampled_steps_per_epoch,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avALvzUp3T_c"
   },
   "source": [
    "If the training process were considering the whole dataset on each gradient update, this oversampling would be basically identical to the class weighting.\n",
    "\n",
    "But when training the model batch-wise, as you did here, the oversampled data provides a smoother gradient signal: Instead of each positive example being shown in one batch with a large weight, they're shown in many different batches each time with a small weight. \n",
    "\n",
    "This smoother gradient signal makes it easier to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klHZ0HV76VC5"
   },
   "source": [
    "### Check training history\n",
    "\n",
    "Note that the distributions of metrics will be different here, because the training data has a totally different distribution from the validation and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YoUGfr1vuivl"
   },
   "outputs": [],
   "source": [
    "plot_metrics(resampled_history )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PuH3A2vnwrh"
   },
   "source": [
    "### Re-train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KFLxRL8eoDE5"
   },
   "source": [
    "Because training is easier on the balanced data, the above training procedure may overfit quickly. \n",
    "\n",
    "So break up the epochs to give the `callbacks.EarlyStopping` finer control over when to stop training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_yn9I26qAHU"
   },
   "outputs": [],
   "source": [
    "resampled_model = make_model()\n",
    "resampled_model.load_weights(initial_weights)\n",
    "\n",
    "# Reset the bias to zero, since this dataset is balanced.\n",
    "output_layer = resampled_model.layers[-1] \n",
    "output_layer.bias.assign([0])\n",
    "\n",
    "resampled_history = resampled_model.fit(\n",
    "    resampled_ds,\n",
    "    # These are not real epochs\n",
    "    steps_per_epoch = 20,\n",
    "    epochs=10*EPOCHS,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UuJYKv0gpBK1"
   },
   "source": [
    "### Re-check training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FMycrpJwn39w"
   },
   "outputs": [],
   "source": [
    "plot_metrics(resampled_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUuE5HOWZiwP"
   },
   "source": [
    "### Evaluate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0fmHSgXxFdW"
   },
   "outputs": [],
   "source": [
    "train_predictions_resampled = resampled_model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_resampled = resampled_model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FO0mMOYUDWFk"
   },
   "outputs": [],
   "source": [
    "resampled_results = resampled_model.evaluate(test_features, test_labels,\n",
    "                                             batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(resampled_model.metrics_names, resampled_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(test_labels, test_predictions_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xYozM1IIITq"
   },
   "source": [
    "### Plot the ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fye_CiuYrZ1U"
   },
   "outputs": [],
   "source": [
    "plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color=colors[0])\n",
    "plot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\n",
    "\n",
    "plot_roc(\"Train Weighted\", train_labels, train_predictions_weighted, color=colors[1])\n",
    "plot_roc(\"Test Weighted\", test_labels, test_predictions_weighted, color=colors[1], linestyle='--')\n",
    "\n",
    "plot_roc(\"Train Resampled\", train_labels, train_predictions_resampled,  color=colors[2])\n",
    "plot_roc(\"Test Resampled\", test_labels, test_predictions_resampled,  color=colors[2], linestyle='--')\n",
    "\n",
    "plt.plot(np.arange(100),np.arange(100),ls=':',alpha=0.5) # x=y\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3o3f0ywl8uqW"
   },
   "source": [
    "## Applying this tutorial to your problem\n",
    "\n",
    "Imbalanced data classification is an inherantly difficult task since there are so few samples to learn from. You should always start with the data first and do your best to collect as many samples as possible and give substantial thought to what features may be relevant so the model can get the most out of your minority class. At some point your model may struggle to improve and yield the results you want, so it is important to keep in mind the context of your problem and the trade offs between different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grqlHkb2AtnV"
   },
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4E3epE98A2Be"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import random\n",
    "#import sklearn \n",
    "#print(sklearn.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style ='whitegrid', palette='bright')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8SgkCzlpCSvw"
   },
   "outputs": [],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PpG-ITRGCSy7"
   },
   "outputs": [],
   "source": [
    "cleaned_df = raw_df.copy()\n",
    "\n",
    "#--- Delete columns we'll not use\n",
    "labels_to_drop = ['date','lights','rv1', 'rv2']\n",
    "cleaned_df = cleaned_df.drop(labels=labels_to_drop, axis=1) # df.drop(): non-fruitful function\n",
    "cleaned_df.head()\n",
    "\n",
    "y = cleaned_df.pop('Class')\n",
    "y.head()\n",
    "\n",
    "df_feat = cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8spVKFjQCS3p"
   },
   "outputs": [],
   "source": [
    "#--- Scale our data: For KNN, it is better to scale the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_feat)\n",
    "scaled_features = scaler.transform(df_feat)\n",
    "scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TumCaDWbA53Z"
   },
   "outputs": [],
   "source": [
    "#--- Split Data: Train and Test data set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_feat #X = scaled_features\n",
    "#y = cancer['target']  -> We already assigned our TARGET to variable 'y'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nOZpAXWcAr8c"
   },
   "outputs": [],
   "source": [
    "#--- KNN\n",
    "#--- set k and find a class \n",
    "\n",
    "random.seed(1)\n",
    "# number of class: According to our problem, we have 2 classes (binary case)\n",
    "k = 2 \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', \n",
    "#                     leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(X_train, y_train)\n",
    "#knn.get_params()\n",
    "knn.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YH5WkPITA51l"
   },
   "outputs": [],
   "source": [
    "pred_tr = knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jdq8UclcA5zD"
   },
   "outputs": [],
   "source": [
    "# predict our Target for the test set\n",
    "pred = knn.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q3mXCOVFA5xK"
   },
   "outputs": [],
   "source": [
    "# Evaluate our result\n",
    "print('Scores without Cross Validation\\n')\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score\n",
    "print('<< Train Errror >>')\n",
    "print('< Confusion Matrix >')\n",
    "print(confusion_matrix(y_train, pred_tr))\n",
    "print('\\n< Classification Report >')\n",
    "print(classification_report(y_train, pred_tr))\n",
    "\n",
    "accuracy = accuracy_score(y_train, pred_tr)\n",
    "#recall = recall_score(y_train, pred_tr) # not using it because our focused target is 0 not 1\n",
    "cm = confusion_matrix(y_train, pred_tr)\n",
    "recall = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "#precision = precision_score(y_train, pred_tr) # not using it because of the same reason above\n",
    "precision = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "\n",
    "print('- Accuracy: {:.2f} \\n- Recall: {:.2f}   (=Sensitivity)\\n- Precision: {:.2f}'.format(accuracy, recall, precision))\n",
    "print('\\n-----------------------------------------------------------------\\n')\n",
    "print('<<Test error>>')\n",
    "\n",
    "print('< Confusion Matrix >')\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print('\\n< Classification Report >')\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "cm1 = confusion_matrix(y_test, pred)\n",
    "recall = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "#precision = precision_score(y_test, pred) # not using it because of the same reason above\n",
    "precision = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
    "#precision = precision_score(y_test, pred)\n",
    "print('- Accuracy: {:.2f} \\n- Recall: {:.2f}   (=Sensitivity)\\n- Precision: {:.2f}'.format(accuracy, recall, precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mxa1XdgJDNz2"
   },
   "outputs": [],
   "source": [
    "#--- Crossvalidation & Grid Search\n",
    "\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'algorithm': ['ball_tree', 'kd_tree', 'brute'], \n",
    "              'metric':['euclidean','manhattan', 'chebyshev', 'minkowski']}\n",
    "# metrics: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n",
    "estimator = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "# k = 5\n",
    "grid_knn = GridSearchCV(estimator, param_grid, cv=5, verbose=3, return_train_score=True)\n",
    "grid_knn.fit(X_train, y_train)\n",
    "grid_knn.cv_results_\n",
    "sorted(grid_knn.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wHw0ZK3kDNvb"
   },
   "outputs": [],
   "source": [
    "grid_knn.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19qLXka2DNnD"
   },
   "outputs": [],
   "source": [
    "#--- Predict with the best estimator after Grid Search and Cross validation\n",
    "pred_knn_tr=grid_knn.best_estimator_.predict(X_train)\n",
    "pred_knn_ts=grid_knn.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_mIZymKDNht"
   },
   "outputs": [],
   "source": [
    "# Results for Train Set\n",
    "\n",
    "print(confusion_matrix(y_train, pred_knn_tr))\n",
    "print(classification_report(y_train, pred_knn_tr))\n",
    "\n",
    "accuracy = accuracy_score(y_train,  pred_knn_tr)\n",
    "#recall = recall_score(y_train, pred_tr) # not using it because our focused target is 0 not 1\n",
    "cm = confusion_matrix(y_train,  pred_knn_tr)\n",
    "recall = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "#precision = precision_score(y_train, pred_tr) # not using it because of the same reason above\n",
    "precision = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "\n",
    "print('- Accuracy: {:.2f} \\n- Recall: {:.2f}   (=Sensitivity)\\n- Precision: {:.2f}'.format(accuracy, recall, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iQgLAuECDNeu"
   },
   "outputs": [],
   "source": [
    "# Results for Test Set\n",
    "\n",
    "print(confusion_matrix(y_test, pred_knn_ts))\n",
    "print(classification_report(y_test, pred_knn_ts))\n",
    "\n",
    "accuracy = accuracy_score(y_test, pred_knn_ts)\n",
    "#recall = recall_score(y_train, pred_tr) # not using it because our focused target is 0 not 1\n",
    "cm = confusion_matrix(y_test, pred_knn_ts)\n",
    "recall = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "#precision = precision_score(y_train, pred_tr) # not using it because of the same reason above\n",
    "precision = cm[1,1]/(cm[0,1]+cm[1,1])\n",
    "\n",
    "print('- Accuracy: {:.2f} \\n- Recall: {:.2f}   (=Sensitivity)\\n- Precision: {:.2f}'.format(accuracy, recall, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T7z0sFLcDNbd"
   },
   "outputs": [],
   "source": [
    "knn_ts_score =[]\n",
    "\n",
    "for i in range(len(grid_knn.cv_results_['params'])):\n",
    "    paramtrs = grid_knn.cv_results_['params'][i]\n",
    "    knn = KNeighborsClassifier(n_neighbors=2, algorithm = paramtrs['algorithm'], metric = paramtrs['metric'])\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred = knn.predict(X_test)\n",
    "    cm1 = confusion_matrix(y_test, pred)\n",
    "    recall = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    knn_ts_score.append(recall)\n",
    "    \n",
    "knn_ts_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3Kq459lDNTQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training Size\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"#0abab5\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#EE82EE\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"#0abab5\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"#EE82EE\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "#--- Plotting\n",
    "cv = 5      # number of folds\n",
    "n_jobs = -1\n",
    "\n",
    "X, y = X_train, y_train\n",
    "\n",
    "title = r\"Learning Curves of KNN)\"\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "estimator = knn\n",
    "\n",
    "plot_learning_curve(estimator, title, X, y, (0.5, 1.01), cv=cv, n_jobs=n_jobs)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxAUyWmiDZGi"
   },
   "outputs": [],
   "source": [
    "def plot_search_results(grid, score):\n",
    " \n",
    "    #--- Results from grid search\n",
    "    results = grid.cv_results_\n",
    "    means_test = results['mean_test_score']\n",
    "    stds_test = results['std_test_score']\n",
    "    means_train = results['mean_train_score']\n",
    "    stds_train = results['std_train_score']\n",
    "    testset = np.array(score)\n",
    "    \n",
    "    \n",
    "    #knn_testset = np.array(knn_ts_score)\n",
    "    #poly_testset = np.array(poly_testset_score)\n",
    "    #sigmoid_testset = np.array(sigmoid_testset_score)\n",
    "    #dtree_testset = np.array(dtree_testset_score)\n",
    "    #gb_testset = np.array(gb_testset_score)\n",
    "\n",
    "    #--- Getting indexes of values per hyper-parameter\n",
    "    masks=[]\n",
    "    masks_names= list(grid.best_params_.keys())\n",
    "    for p_k, p_v in grid.best_params_.items():\n",
    "        masks.append(list(results['param_'+p_k].data==p_v))\n",
    "\n",
    "    params=grid.param_grid\n",
    "    \n",
    "    #--- Ploting results\n",
    "    fig, ax = plt.subplots(1,len(params),sharex='none', sharey='all',figsize=(20,5))\n",
    "    #fig = plot_figure(style_label='classic') #################\n",
    "    fig.suptitle('Scores vs. Parameters')\n",
    "    ax[0].set_ylabel('Sensitivity')\n",
    "    #fig.text(0.04, 0.5, 'Recall', va='center', rotation='vertical')\n",
    "    pram_preformace_in_best = {}\n",
    "    for i, p in enumerate(masks_names):\n",
    "        m = np.stack(masks[:i] + masks[i+1:])\n",
    "        pram_preformace_in_best\n",
    "        best_parms_mask = m.all(axis=0)\n",
    "        best_index = np.where(best_parms_mask)[0]\n",
    "        x = np.array(params[p])\n",
    "        y_1 = np.array(means_train[best_index])\n",
    "        e_1 = np.array(stds_train[best_index])\n",
    "        y_2 = np.array(means_test[best_index])\n",
    "        e_2 = np.array(stds_test[best_index])\n",
    "        y_3 = np.array(testset[best_index])\n",
    "        \n",
    "        ax[i].set_xlabel(p.upper())\n",
    "        ax[i].plot(x,y_1,color=\"#0abab5\", label='Train Score',linestyle='--',marker ='o')\n",
    "        ax[i].plot(x,y_2,color=\"#EE82EE\", label='5fold-Cross-validation Score',linestyle='--',marker ='o')\n",
    "        ax[i].plot(x,y_3,color='b', label='Test Score',linestyle='-',marker ='^')\n",
    "        ax[i].fill_between(x, y_1-e_1,y_1+e_1,alpha=0.1,\n",
    "                     color=\"#0abab5\")\n",
    "        ax[i].fill_between(x, y_2-e_2,y_2+e_2,alpha=0.1,\n",
    "                     color=\"#EE82EE\")\n",
    "        ax[i].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "plot_search_results(grid_knn, knn_ts_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIv4vAbQDZAh"
   },
   "outputs": [],
   "source": [
    "def plot_search_results(grid, score):\n",
    " \n",
    "    #--- Results from grid search\n",
    "    results = grid.cv_results_\n",
    "    means_train = results['mean_train_score']\n",
    "    stds_train = results['std_train_score']\n",
    "    means_test = results['mean_test_score']\n",
    "    stds_test = results['std_test_score']\n",
    "\n",
    "    testset = np.array(score)\n",
    "    \n",
    "    \n",
    "    #knn_testset = np.array(knn_ts_score)\n",
    "    #poly_testset = np.array(poly_testset_score)\n",
    "    #sigmoid_testset = np.array(sigmoid_testset_score)\n",
    "    #dtree_testset = np.array(dtree_testset_score)\n",
    "    #gb_testset = np.array(gb_testset_score)\n",
    "\n",
    "    #--- Getting indexes of values per hyper-parameter\n",
    "    masks=[]\n",
    "    masks_names= list(grid.best_params_.keys())\n",
    "    for p_k, p_v in grid.best_params_.items():\n",
    "        masks.append(list(results['param_'+p_k].data==p_v))\n",
    "\n",
    "    params=grid.param_grid\n",
    "    \n",
    "    #--- Ploting results\n",
    "    fig, ax = plt.subplots(1,len(params),sharex='none', sharey='all',figsize=(20,6))\n",
    "    #fig = plot_figure(style_label='classic') #################\n",
    "    fig.suptitle('Scores vs. Parameters')\n",
    "    ax[0].set_ylabel('Sensitivity')\n",
    "    #fig.text(0.04, 0.5, 'Recall', va='center', rotation='vertical')\n",
    "    pram_preformace_in_best = {}\n",
    "    for i, p in enumerate(masks_names):\n",
    "        m = np.stack(masks[:i] + masks[i+1:])\n",
    "        pram_preformace_in_best\n",
    "        best_parms_mask = m.all(axis=0)\n",
    "        best_index = np.where(best_parms_mask)[0]\n",
    "        #x = np.array(params[p])\n",
    "        x = np.arange(len(np.array(params[p])))\n",
    "        \n",
    "        y_1 = np.array(means_train[best_index])\n",
    "        e_1 = np.array(stds_train[best_index])\n",
    "        y_2 = np.array(means_test[best_index])\n",
    "        e_2 = np.array(stds_test[best_index])\n",
    "        y_3 = np.array(testset[best_index])\n",
    "        \n",
    "        ax[i].set_xticks(x)\n",
    "        ax[i].set_xticklabels(params[p])\n",
    "        al, w = 0.8, 0.2\n",
    "        ax[i].set_xlabel(p.upper())\n",
    "        ax[i].bar(x-w,y_1,color=\"#0abab5\", label='Train Score', alpha=al, width=w)\n",
    "        ax[i].bar(x,y_2,color=\"#EE82EE\", label='5fold-Cross-validation Score', alpha=al, width=w)\n",
    "        ax[i].bar(x+w,y_3,color='b', label='Test Score', alpha=al, width=w)\n",
    "        #ax[i].fill_between(x, y_1-e_1,y_1+e_1,alpha=0.1,color=\"#0abab5\")\n",
    "        #ax[i].fill_between(x, y_2-e_2,y_2+e_2,alpha=0.1,color=\"#EE82EE\")\n",
    "        ax[i].legend(loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "plot_search_results(grid_knn, knn_ts_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FaNlQZqVDYzh"
   },
   "outputs": [],
   "source": [
    "results = grid_knn.cv_results_\n",
    "means_train = results['mean_train_score']\n",
    "means_train\n",
    "grid_knn.best_params_.keys()\n",
    "grid_knn.best_params_['algorithm']\n",
    "grid_knn.best_params_['metric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQqP1fyFDgxI"
   },
   "outputs": [],
   "source": [
    "# elbow method to choose better K value\n",
    "error_rate = []\n",
    "\n",
    "for i in range(1,40):\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(range(1,40), error_rate, color='blue', ls= 'dashed',marker='o', markersize=5)\n",
    "plt.title('Error Rate vs K Value(the number of Nodes)')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dUeKVCYTbcyT"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors on the snippets of the Tensorflow Codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rruk6K8A8zjl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_Dataset1_GaHyunLee.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
